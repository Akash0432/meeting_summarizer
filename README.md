# ğŸ§  Meeting Summarizer (Whisper.cpp + Ollama + Gradio)

This project is an **AI-powered Meeting Summarizer** that converts audio recordings of meetings into **text transcripts** using **Whisper.cpp**, and then **summarizes** them using a **local Ollama LLM model** â€” all through a simple and beautiful **Gradio web interface**.

---

## ğŸš€ Features

âœ… **Automatic Speech-to-Text** using `whisper.cpp`
âœ… **Smart Summarization** using your preferred **Ollama** model
âœ… **Context-Aware Summaries** (optional context input)
âœ… **Downloadable Transcript File**
âœ… **Fully Local Processing** â€“ No API keys, no cloud costs
âœ… **Cross-Platform Setup Script** for quick installation

---

## ğŸ§© Project Structure

```
meeting-summarizer/
â”‚
â”œâ”€â”€ main.py                    # Main Python app with Gradio interface
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ setup.sh                    # Auto setup script (builds whisper.cpp, installs deps)
â”œâ”€â”€ whisper.cpp/                # Cloned whisper.cpp repo
â”‚   â”œâ”€â”€ models/                 # Stores Whisper model binaries (.bin)
â”‚   â””â”€â”€ main                    # whisper.cpp binary (after build)
â””â”€â”€ README.md                   # Youâ€™re reading it now!
```

---

## âš™ï¸ Installation & Setup

### 1ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/<your-username>/meeting-summarizer.git
cd meeting-summarizer
```

---

### 2ï¸âƒ£ Run the Setup Script

Run the setup file to automatically:

* Create a Python virtual environment
* Install all dependencies (`gradio`, `requests`, etc.)
* Install and build `whisper.cpp`
* Download the chosen Whisper model
* Check for and install `ffmpeg` if missing
* Finally, run the app ğŸš€

```bash
bash setup.sh
```

---

### 3ï¸âƒ£ Manual Setup (Alternative)

If you prefer to install manually:

#### ğŸ Create & Activate Virtual Environment

```bash
python3 -m venv .venv
source .venv/bin/activate
```

#### ğŸ“† Install Dependencies

```bash
pip install -r requirements.txt
```

#### ğŸ§  Clone & Build whisper.cpp

```bash
git clone https://github.com/ggerganov/whisper.cpp.git
cd whisper.cpp
make
cd ..
```

#### ğŸ“… Download Whisper Model

```bash
./whisper.cpp/models/download-ggml-model.sh small
```

(You can replace `small` with `base`, `medium`, or `large`.)

---

## ğŸ¥ª Running the App

Once everything is set up, launch the Gradio web interface:

```bash
python main.py
```

Gradio will display a local link in your terminal:

```
Running on local URL: http://127.0.0.1:7860
```

Open it in your browser to start using the summarizer.

---

## ğŸ–¥ï¸ How It Works

1. **Upload an audio file** (e.g., `.mp3`, `.wav`, `.m4a`, `.flac`)
2. The audio is **converted to 16kHz mono WAV** using FFmpeg
3. `whisper.cpp` generates the **transcript text file**
4. Ollama model (like `llama3`, `mistral`, etc.) summarizes the transcript
5. You can **read, copy, and download** both the summary and transcript

---

## ğŸ§  Example Flow

| Step | Action                 | Result                                        |
| ---- | ---------------------- | --------------------------------------------- |
| ğŸ¤   | Upload `meeting.mp3`   | Audio converted â†’ Transcript generated        |
| ğŸ“   | Choose Whisper model   | `small` (default)                             |
| ğŸ’¬   | Choose LLM model       | `llama3`, `mistral`, etc.                     |
| ğŸ“š   | (Optional) Add Context | â€œTeam standup about product roadmapâ€          |
| âš¡    | Click Submit           | Summary generated and transcript downloadable |

---

## ğŸ§© Dependencies

All dependencies are handled automatically, but here are the key packages:

| Package                 | Purpose              |
| ----------------------- | -------------------- |
| `gradio`                | Web UI               |
| `requests`              | Ollama API calls     |
| `ffmpeg`                | Audio preprocessing  |
| `torch`, `transformers` | (for LLMs if needed) |
| `whisper.cpp`           | Local ASR model      |

---

## ğŸ§± Requirements

* Python â‰¥ 3.8
* `ffmpeg` installed
* `ollama` running locally (`ollama serve`)
* Sufficient storage for model binaries (~2â€“3 GB for large models)

---

## ğŸ§  Environment Variables

| Variable            | Description                 | Default                  |
| ------------------- | --------------------------- | ------------------------ |
| `OLLAMA_SERVER_URL` | URL for Ollama server       | `http://localhost:11434` |
| `WHISPER_MODEL_DIR` | Directory of Whisper models | `./whisper.cpp/models`   |

---

## ğŸ§© Available Models

### ğŸ•¡ Whisper Models

* `base`
* `small`
* `medium`
* `large`
* `large-V3`

### ğŸ’¬ LLM Models (Ollama)

Run this to check available models:

```bash
ollama list
```

---

## ğŸ› ï¸ Troubleshooting

| Problem                                          | Solution                                     |
| ------------------------------------------------ | -------------------------------------------- |
| âŒ â€œFailed to retrieve models from Ollama serverâ€ | Ensure Ollama is running (`ollama serve`)    |
| âŒ `ffmpeg` not found                             | Install manually (`sudo apt install ffmpeg`) |
| âŒ `whisper.cpp/main: not found`                  | Run `make` inside `whisper.cpp` again        |
| âŒ Permission denied                              | Run `chmod +x setup.sh`                      |

---

## ğŸ“„ License

MIT License Â© 2025 â€” Akash Pandranki

Feel free to use, modify, and distribute this project.

---

## ğŸ’¡ Credits

* [ggerganov/whisper.cpp](https://github.com/ggerganov/whisper.cpp)
* [Ollama](https://ollama.ai/)
* [Gradio](https://gradio.app/)
